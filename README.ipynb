{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 2 Reinforcement Learning Agents to play Tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agents](./images/multi_agents.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I worked with an environment similar but not identical to the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment on the Unity ML-Agents Github page.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "   - After each episode, I added up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. Then the maximum of these 2 scores was taken.\n",
    "   - This yields a single score for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup and Dependencies Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions in the [DRLND GitHub repository](https://github.com/udacity/deep-reinforcement-learning#dependencies) to set up your Python environment. These instructions can be found in ```README.md``` at the root of the repository. By following these instructions, you will install PyTorch, the ML-Agents toolkit, and a few more Python packages required for this complete the project.\n",
    "\n",
    "You can also follow these steps to install the dependencies:\n",
    "\n",
    "1. **Create (and activate) a new environment with Python 3.6**\n",
    "<br>\n",
    "\n",
    ">> - Linux or Mac: \n",
    "```bash\n",
    "conda create --name Tennis_Agents python=3.6\n",
    "source activate Tennis_Agents\n",
    "```\n",
    "- Windows: \n",
    "```bash\n",
    "conda create --name Tennis_Agents python=3.6 \n",
    "activate Tennis_Agents\n",
    "```\n",
    "\n",
    "2. **Install PyTorch and and other dependencies**\n",
    ">> - [Pytorch](https://pytorch.org/): Make sure you select the Version that supports your OS and system Architecture. Follow the instructions afterwards.\n",
    ">>- Install NumPy and Matplotlib\n",
    "```bash\n",
    "conda install numpy, matplotlib```\n",
    "    \n",
    "\n",
    "(For Windows users) The ML-Agents toolkit supports Windows 10. While it might be possible to run the ML-Agents toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents toolkit has not been tested on a Windows VM such as Bootcamp or Parallels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, you will not need to install Unity - [Udacity](https://udacity.com) already built the environment, and you can download it from one of the links below. You need only select the environment that matches your operating system:\n",
    "\n",
    "\n",
    "- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n",
    "- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n",
    "- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n",
    "- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n",
    "\n",
    "> (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n",
    "\n",
    "> (_For AWS_) If you'd like to train the agent on AWS (and have not enabled a virtual screen), then please use this [link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip) to obtain the \"headless\" version of the environment. You will not be able to watch the agent without enabling a virtual screen, but you will be able to train the agent. (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the Linux operating system above._)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Notebok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have followed the instructions above carefully, you are now ready to run the my notebook.\n",
    "\n",
    "> Clone this project to have access to it locally, Unzip it and move it to where it can be accessible by the environment.\n",
    "\n",
    "You can also choose to run the project on Udacity Workspace, but the provided workspace doesn't allow you to use simulator. You can use the simulator locally to watch the agent.\n",
    "\n",
    ">- You can either use the pretrained weights or \n",
    "- Run the ```Tennis.ipynb``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
